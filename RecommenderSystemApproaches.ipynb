{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f080e201-babd-4589-bdba-ef8366f6f767",
   "metadata": {},
   "source": [
    "# Recommender System Approaches\n",
    "This notebooks was written for the purpose of experimenting with different algorithms in recommender systems. \n",
    "\n",
    "We reserve the following assumptions for all the algorithms:\n",
    "- Implicit feedback with binary data. We only know if a user interacted with an item (1), or not (0).\n",
    "- Data is split using the user-based, time-ordered, leave-one-out strategy. This means that for each users we sort their interactions and split into train/val/test in the following way. The last interaction is the test data. The one before is the validation data. The rest is the training data.\n",
    "- We experiment on 1 month extract of LFM2b (last month)\n",
    "- Evaluate on NDCG and Hit Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "061fee2e-5da6-425a-b199-9e7fac73b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protorec_dataset import get_protorecdataset_dataloader\n",
    "from utilities.utils import reproducible, print_results\n",
    "from utilities.eval import Evaluator\n",
    "\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse import linalg as sp_lin\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import bottleneck as bn\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.notebook import tqdm,trange\n",
    "\n",
    "from torch import nn \n",
    "from utilities.utils import general_weight_init\n",
    "\n",
    "SEED = 1391075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb8181d-979f-4ea1-9baa-ce84123f4342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Built ProtoRecDataset module \n",
      "- data_path: ./data/lfm2b-1m \n",
      "- n_users: 3555 \n",
      "- n_items: 77985 \n",
      "- n_interactions: 870255 \n",
      "- split_set: train \n",
      "- n_neg: 10 \n",
      "- neg_strategy: uniform \n",
      "\n",
      "Loading data\n",
      "Built ProtoRecDataset module \n",
      "- data_path: ./data/lfm2b-1m \n",
      "- n_users: 3555 \n",
      "- n_items: 77985 \n",
      "- n_interactions: 3555 \n",
      "- split_set: val \n",
      "- n_neg: 99 \n",
      "- neg_strategy: uniform \n",
      "\n",
      "Loading data\n",
      "Built ProtoRecDataset module \n",
      "- data_path: ./data/lfm2b-1m \n",
      "- n_users: 3555 \n",
      "- n_items: 77985 \n",
      "- n_interactions: 3555 \n",
      "- split_set: test \n",
      "- n_neg: 99 \n",
      "- neg_strategy: uniform \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_protorecdataset_dataloader(\n",
    "                    data_path='./data/lfm2b-1m',\n",
    "                    split_set='train',\n",
    "                    n_neg=10,\n",
    "                    neg_strategy='uniform',\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    num_workers=8,\n",
    ")\n",
    "val_loader = get_protorecdataset_dataloader(\n",
    "                    data_path='./data/lfm2b-1m',\n",
    "                    split_set='val',\n",
    "                    n_neg=99,\n",
    "                    neg_strategy='uniform',\n",
    "                    batch_size=64,\n",
    "                    num_workers=8,\n",
    ")\n",
    "\n",
    "test_loader = get_protorecdataset_dataloader(\n",
    "                    data_path='./data/lfm2b-1m',\n",
    "                    split_set='test',\n",
    "                    n_neg=99,\n",
    "                    neg_strategy='uniform',\n",
    "                    batch_size=64,\n",
    "                    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3c3dd-00bb-4215-807e-7639d28b3366",
   "metadata": {},
   "source": [
    "# Random Algorithm\n",
    "For each user, recommend items sampled u.a.r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bba596e-3f56-4caa-b7bb-f8b9fb8fb52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.007\n",
      "hit_ratio@1 : 0.007\n",
      "ndcg@3     : 0.020\n",
      "hit_ratio@3 : 0.030\n",
      "ndcg@5     : 0.028\n",
      "hit_ratio@5 : 0.049\n",
      "ndcg@10    : 0.046\n",
      "hit_ratio@10 : 0.106\n",
      "ndcg@50    : 0.127\n",
      "hit_ratio@50 : 0.495\n"
     ]
    }
   ],
   "source": [
    "reproducible(SEED)\n",
    "\n",
    "evaluator = Evaluator(test_loader.dataset.n_users)\n",
    "\n",
    "for u_idxs, i_idxs, labels in test_loader:\n",
    "    n_batch_users, n_batch_items = i_idxs.shape\n",
    "    # Generate random scores\n",
    "    out = np.random.rand(n_batch_users,n_batch_items)\n",
    "    \n",
    "    evaluator.eval_batch(out)\n",
    "\n",
    "metrics_values = evaluator.get_results()\n",
    "print_results(metrics_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2c8bc-4a96-4960-8857-fe6c83e9c862",
   "metadata": {},
   "source": [
    "# Popular Items\n",
    "For each item, assign a score that depends on the popularity of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea733399-5c8b-4194-aece-c0fac74aeb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.046\n",
      "hit_ratio@1 : 0.046\n",
      "ndcg@3     : 0.081\n",
      "hit_ratio@3 : 0.106\n",
      "ndcg@5     : 0.102\n",
      "hit_ratio@5 : 0.156\n",
      "ndcg@10    : 0.132\n",
      "hit_ratio@10 : 0.248\n",
      "ndcg@50    : 0.205\n",
      "hit_ratio@50 : 0.585\n"
     ]
    }
   ],
   "source": [
    "reproducible(SEED)\n",
    "\n",
    "evaluator = Evaluator(test_loader.dataset.n_users)\n",
    "\n",
    "for u_idxs, i_idxs, labels in test_loader:\n",
    "    \n",
    "    # Look-up of the popularity (compared to the training data)\n",
    "    out = test_loader.dataset.pop_distribution[i_idxs]\n",
    "    \n",
    "    evaluator.eval_batch(out)\n",
    "\n",
    "metrics_values = evaluator.get_results()\n",
    "print_results(metrics_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3263ab-b207-4940-8f1f-d8c8e1755b11",
   "metadata": {},
   "source": [
    "# SVD (constrainted matrix factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785d93d-903d-46c1-94b4-5719dd92a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.188\n",
      "hit_ratio@1 : 0.188\n",
      "ndcg@3     : 0.272\n",
      "hit_ratio@3 : 0.335\n",
      "ndcg@5     : 0.307\n",
      "hit_ratio@5 : 0.419\n",
      "ndcg@10    : 0.350\n",
      "hit_ratio@10 : 0.551\n",
      "ndcg@50    : 0.419\n",
      "hit_ratio@50 : 0.862\n"
     ]
    }
   ],
   "source": [
    "csr_matrix = train_loader.dataset.csr_matrix\n",
    "csr_matrix = csr_matrix.asfptype() # casting to float\n",
    "\n",
    "u,s,vt = sp_lin.svds(csr_matrix,k=100)\n",
    "\n",
    "users_factors = u * s\n",
    "items_factors = vt.T\n",
    "\n",
    "reproducible(SEED)\n",
    "\n",
    "evaluator = Evaluator(test_loader.dataset.n_users)\n",
    "\n",
    "for u_idxs, i_idxs, labels in test_loader:\n",
    "    \n",
    "    \n",
    "    batch_users = users_factors[u_idxs]\n",
    "    batch_items = items_factors[i_idxs]\n",
    "    \n",
    "    out = (batch_items * batch_users[:,None,:]).sum(axis=-1) # Carrying out the dot product\n",
    "    \n",
    "    \n",
    "    evaluator.eval_batch(out)\n",
    "\n",
    "metrics_values = evaluator.get_results()\n",
    "print_results(metrics_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24727d28-48eb-4978-9547-1284ad5985d5",
   "metadata": {},
   "source": [
    "# User-based KNN (Collaborative Filtering)\n",
    "There are numerous extensions of KNN. Here we just consider variation in terms of the similarity functions used.\n",
    "In the end, we generate a n_user x n_user similarity matrix. The prediction for an item i and and user u is given by the weighted average of the similarities of user u and its top-k neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d70a111-6ea5-4277-b7af-4a4cd0ed9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix = train_loader.dataset.csr_matrix\n",
    "\n",
    "def take_only_top_k(sim_mtx,k=100):\n",
    "    # This method slims down the similarity matrix by only picking the top-k most similar items. It also allows a faster computation of the prediction (as you can see in evaluate_user_knn and evaluate_item_knn)\n",
    "    \n",
    "    new_data = []\n",
    "    new_indices = []\n",
    "    new_indptr = [0]\n",
    "    \n",
    "    n_entities = sim_mtx.shape[0]\n",
    "    \n",
    "    cumulative_sum = 0\n",
    "    \n",
    "    for idx in range(n_entities):\n",
    "        start_idx = sim_mtx.indptr[idx]\n",
    "        end_idx = sim_mtx.indptr[idx + 1]\n",
    "        \n",
    "        data = sim_mtx.data[start_idx:end_idx]\n",
    "        ind = sim_mtx.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Avoding taking the user/item itself\n",
    "        self_idx = np.where(ind == idx)[0][0]\n",
    "        data[self_idx] = 0.\n",
    "        \n",
    "        top_k_indxs = np.argsort(-data)[:k] \n",
    "        \n",
    "        top_k_data = data[top_k_indxs]\n",
    "        top_k_indices = ind[top_k_indxs]\n",
    "        \n",
    "        new_data +=  list(top_k_data)\n",
    "        new_indices += list(top_k_indices)\n",
    "        \n",
    "        cumulative_sum += len(top_k_data)\n",
    "        \n",
    "        new_indptr.append(cumulative_sum)\n",
    "        \n",
    "        \n",
    "    return sp.csr_matrix((new_data,new_indices,new_indptr),shape=sim_mtx.shape)\n",
    "\n",
    "\n",
    "def evaluate_user_knn(matrix,sim_fun,k=100):\n",
    "    # matrix is the user x item binary matrix\n",
    "    # sim_fun is the function that carries out the similarity between entities in the matrix (always considering the entities on the rows)\n",
    "    # k is the number of neighbours\n",
    "    \n",
    "    \n",
    "    sim_mtx = take_only_top_k(sim_fun(matrix),k)\n",
    "    \n",
    "    pred_mtx = sim_mtx @ matrix \n",
    "    pred_mtx = pred_mtx.toarray() # Ugly solution but I spent alreday too much on this\n",
    "\n",
    "    reproducible(SEED)\n",
    "\n",
    "    evaluator = Evaluator(test_loader.dataset.n_users)\n",
    "\n",
    "    for u_idxs, i_idxs, labels in test_loader:\n",
    "\n",
    "        out = pred_mtx[u_idxs[:,None],i_idxs]\n",
    "        evaluator.eval_batch(out)\n",
    "\n",
    "    metrics_values = evaluator.get_results()\n",
    "    print_results(metrics_values)\n",
    "    \n",
    "csr_matrix = train_loader.dataset.csr_matrix\n",
    "\n",
    "def evaluate_item_knn(matrix,sim_fun,k=100):\n",
    "    \n",
    "    sim_mtx = take_only_top_k(sim_fun(matrix.T),k)\n",
    "    \n",
    "    pred_mtx = matrix @ sim_mtx.T # Note that the item matrix has to be transposed since we took the top-k for each row!\n",
    "    pred_mtx = pred_mtx.toarray() # Ugly solution but I spent alreday too much on this\n",
    "    \n",
    "    reproducible(SEED)\n",
    "\n",
    "    evaluator = Evaluator(test_loader.dataset.n_users)\n",
    "\n",
    "    for u_idxs, i_idxs, labels in test_loader:\n",
    "        \n",
    "        out = pred_mtx[u_idxs[:,None],i_idxs]\n",
    "        evaluator.eval_batch(out)\n",
    "\n",
    "    metrics_values = evaluator.get_results()\n",
    "    print_results(metrics_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d8735-a0a6-49fb-a7b6-b08b711e644b",
   "metadata": {},
   "source": [
    "## Similarity Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7491bf-190a-410d-b00f-31992f203778",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity\n",
    "Defined a $\\frac{|I_x \\cap I_y|}{|I_x \\cup I_y|}$ where $I_x$ and $I_y$ are the set of items interacted by user x and y respectively. \n",
    "\n",
    "**Attempt to a intuitive explanation/reading the formula**:\n",
    "Two users are considered similar if they have a high number of items that both have consumed. This quantity is normalized by their 'total' items consumed. The last part allows to compare users with different number of items consumed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577aeb5b-a46d-4412-b290-a5776aca2db8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.224\n",
      "hit_ratio@1 : 0.224\n",
      "ndcg@3     : 0.309\n",
      "hit_ratio@3 : 0.372\n",
      "ndcg@5     : 0.341\n",
      "hit_ratio@5 : 0.451\n",
      "ndcg@10    : 0.378\n",
      "hit_ratio@10 : 0.566\n",
      "ndcg@50    : 0.403\n",
      "hit_ratio@50 : 0.662\n"
     ]
    }
   ],
   "source": [
    "def compute_jaccard_sim_mtx(matrix):\n",
    "    jaccard_sim_mtx = (matrix @ matrix.T)\n",
    "\n",
    "    counts = np.array(matrix.sum(axis=1)).squeeze()\n",
    "    try:\n",
    "        union = counts.T + counts - jaccard_sim_mtx # unfortunately this consumes too much memory since it materialize a n_entities x n_entities matrix.\n",
    "        jaccard_sim_mtx = sp.csr_matrix(jaccard_sim_mtx / union)\n",
    "    except Exception as e:\n",
    "        print('Resorting to slower method (never checked if it terminates though)')\n",
    "        rows_nz,cols_nz = jaccard_sim_mtx.nonzero()\n",
    "        jaccard_sim_mtx[rows_nz,cols_nz] = jaccard_sim_mtx[rows_nz,cols_nz] / (counts[rows_nz] + counts[cols_nz] - jaccard_sim_mtx[rows_nz,cols_nz])    \n",
    "    \n",
    "    return jaccard_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,compute_jaccard_sim_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8d3ca-0450-482a-82fd-c4a7ee51a3c6",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Defined as $\\frac{\\sum_i u_x^i \\cdot u_y^i}{\\sqrt{\\sum_i {u_x^i}^2} \\sqrt{\\sum_i {u_y^i}^2}}$ where $u_x$ and $u_y$ are the row vectors of the user-item interaction matrix\n",
    "\n",
    "**Attempt to a intuitive explanation/reading the formula**: (Note that in the case of binary data as it is now, the cosine similarity is ~similar to jaccard. The difference is in the denominator: while for jaccard you simiply sum the elements in the union, for cosine we sum the square roots of the sum of the elements in each set.). The cosine similarity measures the angle between two vectors. Assuming that each item represent a specific dimension, the cosine ~checks how these vectors are oriented in the multidimensional space. If the vectors are oriented in the same direction (regardless of their intensity/length), then they are similar. If they are oriented in opposite directions, then they are dissimilar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60296545-af91-409a-af5d-b5f0f7e1ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.226\n",
      "hit_ratio@1 : 0.226\n",
      "ndcg@3     : 0.321\n",
      "hit_ratio@3 : 0.390\n",
      "ndcg@5     : 0.354\n",
      "hit_ratio@5 : 0.471\n",
      "ndcg@10    : 0.393\n",
      "hit_ratio@10 : 0.590\n",
      "ndcg@50    : 0.434\n",
      "hit_ratio@50 : 0.763\n"
     ]
    }
   ],
   "source": [
    "def compute_cosine_sim_mtx(matrix):\n",
    "    \n",
    "    norms = sp_lin.norm(matrix,axis=1)\n",
    "\n",
    "    normalized_matrix = sp.csr_matrix((matrix.T / norms).T)\n",
    "\n",
    "    cosine_sim_mtx = normalized_matrix @ normalized_matrix.T\n",
    "    \n",
    "    return cosine_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,compute_cosine_sim_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62267955-a93c-4731-a01d-fa3e65f2b781",
   "metadata": {},
   "source": [
    "### Pearson Correlation\n",
    "Defined as $\\frac{\\sum_i (u_x^i - \\bar{u_x}) \\cdot (u_y^i - \\bar{u_y})}{\\sqrt{\\sum_i {(u_x^i - \\bar{u_x})}^2} \\sqrt{\\sum_i {(u_y^i - \\bar{u_y})}^2}}$ where $u_x$ and $u_y$ are the row vectors of the user-item interaction matrix and $\\bar{u_x}$ and $\\bar{u_y}$ the respective means\n",
    "\n",
    "**Attempt to a intuitive explanation/reading the formula**: In general, pearson correlation evaluates if two measurments are linearly dependent to each other (i.e. if you can write Y=aX + b where X,Y are user vector ratings and a,b are parameters). The definition is similar to the cosine similarity but by subtracting the mean of the measurments , computed over *all* the items. Note that, otherwise, the mean would be 1, and thus leading to 0 everywhere when removing the mean (1-1=0). This mean, then, should take into account some information about the number consumed by the users (the higher the number of items consumed, the bigger the mean), similarly what the denominator does for the jaccard similarity. The mean is then removed by the vectors (only non-zero entries) and the definition of cosine similarity is applied. The mean is removed only to these entries because... performance? Removing the mean to 0s makes them go below 0. Does it make sense at all to have a negative similarity? Looking at the results, it seems that the metrics  are even better for the dense definition below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55985e28-15f3-4c6a-8cfb-15d219d4752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.226\n",
      "hit_ratio@1 : 0.226\n",
      "ndcg@3     : 0.321\n",
      "hit_ratio@3 : 0.390\n",
      "ndcg@5     : 0.354\n",
      "hit_ratio@5 : 0.471\n",
      "ndcg@10    : 0.393\n",
      "hit_ratio@10 : 0.590\n",
      "ndcg@50    : 0.434\n",
      "hit_ratio@50 : 0.763\n"
     ]
    }
   ],
   "source": [
    "def compute_pearson_sim_mtx(matrix):\n",
    "\n",
    "    means = np.array(matrix.mean(axis=1)).flatten()\n",
    "    matrix_no_mean = matrix.copy().asfptype()\n",
    "\n",
    "    for indx in range(matrix.shape[0]):\n",
    "        matrix_no_mean.data[matrix.indptr[indx]:matrix.indptr[indx+1]] -= means[indx]\n",
    "\n",
    "    norms_no_mean = sp_lin.norm(matrix_no_mean,axis=1)\n",
    "\n",
    "    normalized_matrix_no_mean = sp.csr_matrix((matrix_no_mean.T / norms_no_mean).T)\n",
    "\n",
    "    pearson_sim_mtx = normalized_matrix_no_mean @ normalized_matrix_no_mean.T\n",
    "\n",
    "    return pearson_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,compute_pearson_sim_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319e2bc0-3232-4823-b6d0-e640d70aa4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.226\n",
      "hit_ratio@1 : 0.226\n",
      "ndcg@3     : 0.322\n",
      "hit_ratio@3 : 0.391\n",
      "ndcg@5     : 0.355\n",
      "hit_ratio@5 : 0.473\n",
      "ndcg@10    : 0.393\n",
      "hit_ratio@10 : 0.590\n",
      "ndcg@50    : 0.434\n",
      "hit_ratio@50 : 0.758\n"
     ]
    }
   ],
   "source": [
    "# REMOVING THE MEAN TO ALL ENTRIES\n",
    "### Watch out! We make dense a 3555 x 77985 matrix! Make sure you have enough memory!\n",
    "\n",
    "def compute_pearson_dense_sim_mtx(matrix):\n",
    "    \n",
    "    means = np.array(matrix.mean(axis=1)).flatten()\n",
    "\n",
    "    matrix_no_mean = matrix - means[:,None]\n",
    "\n",
    "    norms_no_mean = sc.linalg.norm(matrix_no_mean,axis=1)\n",
    "\n",
    "    normalized_matrix_no_mean = (matrix_no_mean.T / norms_no_mean).T\n",
    "\n",
    "    pearson_dense_sim_mtx = sp.csr_matrix(normalized_matrix_no_mean @ normalized_matrix_no_mean.T)\n",
    "    return pearson_dense_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,compute_pearson_dense_sim_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de655645-627a-4525-9e22-29d1539a7300",
   "metadata": {},
   "source": [
    "### Asymmetric Cosine Similarity\n",
    "Defined as $\\frac{|I_x \\cap I_y|}{|I_x|^\\alpha |I_y|^{1-\\alpha}}$ where $I_x$ and $I_y$ are the set of items interacted by user x and y respectively and $\\alpha$ is a hyperparameter. (see also [here](https://dl.acm.org/doi/pdf/10.1145/2507157.2507189))\n",
    "\n",
    "**Attempt to a intuitive explanation/reading the formula**: The cosine similarity can be generalized in such a way to give different weght to the different cardilnalities of the  item sets. The paper reports this example for the items. Imagine you are comparing two songs and you want to estimates their similarity. Assume that song A is a popular song from (changing the band) Muse e.g. Madness and song B a more unknown song by them e.g. Unintended. If we know that a user consumed B, it is likely that the user is a Muse's fan and will also consume A. If we know that user consumed A, it is not clear if the user just listened to this popular song or is a fan of Muse (and consume song B). This leads to: song B is more similar to song A than song A is to B. For this reason, we scale the weight given a specific alpha. As an example, consider that Song A got 100 users, Song B 12 users and only 10 users listened to both Song A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e13a4086-1b7b-4b47-a9ff-cfc68c927595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.214\n",
      "hit_ratio@1 : 0.214\n",
      "ndcg@3     : 0.305\n",
      "hit_ratio@3 : 0.373\n",
      "ndcg@5     : 0.342\n",
      "hit_ratio@5 : 0.461\n",
      "ndcg@10    : 0.379\n",
      "hit_ratio@10 : 0.577\n",
      "ndcg@50    : 0.430\n",
      "hit_ratio@50 : 0.796\n"
     ]
    }
   ],
   "source": [
    "def compute_asymmcosine_sim_mtx(alpha,matrix):\n",
    "    \n",
    "    sums = np.squeeze(np.asarray(matrix.sum(axis=1)))\n",
    "\n",
    "    sums_alpha = np.power(sums,alpha)\n",
    "    sums_1_min_alpha = np.power(sums,1-alpha)\n",
    "\n",
    "    denominator = np.outer(sums_alpha,sums_1_min_alpha)\n",
    "\n",
    "    asymmetric_sim_mtx = sp.csr_matrix((matrix @ matrix.T)/denominator)\n",
    "    return asymmetric_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,partial(compute_asymmcosine_sim_mtx,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de8d2e-e44b-4117-b5c5-120753e1525d",
   "metadata": {},
   "source": [
    "### Sørensen–Dice Coefficient\n",
    "Defined as $2\\frac{|I_x \\cap I_y|}{|I_x| + |I_y|}$ where $I_x$ and $I_y$ are the set of items interacted by user x and y respectively. (see also [here](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient))\n",
    "\n",
    "**Attempt to a intuitive explanation/reading the formula**:\n",
    "No real explanation for this one. It basically just give twice the weight to the shared information. (It is also very similar to Jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3311505e-8e46-4537-ab2b-ca63628f1ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.228\n",
      "hit_ratio@1 : 0.228\n",
      "ndcg@3     : 0.319\n",
      "hit_ratio@3 : 0.384\n",
      "ndcg@5     : 0.358\n",
      "hit_ratio@5 : 0.479\n",
      "ndcg@10    : 0.396\n",
      "hit_ratio@10 : 0.596\n",
      "ndcg@50    : 0.433\n",
      "hit_ratio@50 : 0.753\n"
     ]
    }
   ],
   "source": [
    "def compute_sorensendice_sim_mtx(matrix):\n",
    "    \n",
    "    intersection = (matrix @ matrix.T)\n",
    "\n",
    "    counts = matrix.sum(axis=1)\n",
    "    counts_sum = counts + counts.T\n",
    "\n",
    "\n",
    "    sorensedice_sim_mtx = sp.csr_matrix(2 * intersection / counts_sum)\n",
    "    return sorensedice_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,compute_sorensendice_sim_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4499bab-2e36-44a6-ae53-32df0661676b",
   "metadata": {},
   "source": [
    "### Tversky Index\n",
    "Defined as $\\frac{|I_x \\cap I_y|}{|I_x \\cap I_y| + \\alpha|I_x - I_y| + \\beta|I_y - I_x|}$ where $I_x$ and $I_y$ are the set of items interacted by user x and y respectively and $\\alpha$ and $\\beta$ are hyperparameters. (see also [here](https://en.wikipedia.org/wiki/Tversky_index))\n",
    "\n",
    "**Attempt to a intuitive explanation/reading the formula**:\n",
    "As Asymmetric Cosine, the Tversky index is an asymmetric similarity function. It basically gives a 'different' weighting scheme for the common/ not-in-common items compared to asymmetric cosine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24ed60e-2ce1-433f-a5f2-54efe828bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.228\n",
      "hit_ratio@1 : 0.228\n",
      "ndcg@3     : 0.318\n",
      "hit_ratio@3 : 0.382\n",
      "ndcg@5     : 0.356\n",
      "hit_ratio@5 : 0.475\n",
      "ndcg@10    : 0.394\n",
      "hit_ratio@10 : 0.593\n",
      "ndcg@50    : 0.434\n",
      "hit_ratio@50 : 0.761\n"
     ]
    }
   ],
   "source": [
    "def compute_tversky_sim_mtx(alpha,beta,matrix):\n",
    "    \n",
    "    intersection = (matrix @ matrix.T)\n",
    "\n",
    "    counts = matrix.sum(axis=1)\n",
    "    complement = counts - intersection\n",
    "\n",
    "    tversky_sim_mtx = sp.csr_matrix(intersection/(intersection + alpha * complement + beta * complement.T))\n",
    "    return tversky_sim_mtx\n",
    "\n",
    "evaluate_user_knn(csr_matrix,partial(compute_tversky_sim_mtx,0.8,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15315a85-4239-460d-a40d-43bc5f610130",
   "metadata": {},
   "source": [
    "# Item-based KNN (Collaborative Filtering)\n",
    "We generate a n_items x n_items similarity matrix. The prediction for an item i and and user u is given by the weighted average of the similarities of item i and its top-k neighbours that also were consumed by user u. \n",
    "\n",
    "**Most of these functions should be further optimized in order to work for huge similarity matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782bb4a-53d9-48e0-9a44-dc0fcda99a0e",
   "metadata": {},
   "source": [
    "### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04bdbf76-f3f4-42f5-8e01-b139a25c7ee3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resorting to slower method\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22210c7feae41ad94bfe7e060143733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39203/4099266959.py\u001b[0m in \u001b[0;36mcompute_jaccard_sim_mtx\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mjaccard_sim_mtx\u001b[0m \u001b[0;31m#unfortunately this consumes too much memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mjaccard_sim_mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaccard_sim_mtx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 45.3 GiB for an array with shape (77985, 77985) and data type int64",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39203/333364262.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_item_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompute_jaccard_sim_mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_39203/3374382549.py\u001b[0m in \u001b[0;36mevaluate_item_knn\u001b[0;34m(matrix, sim_fun, k)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_item_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0msim_mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_only_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mpred_mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0msim_mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# Note that the item matrix has to be transposed since we took the top-k for each row!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39203/4099266959.py\u001b[0m in \u001b[0;36mcompute_jaccard_sim_mtx\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_idxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mjaccard_sim_mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_sim_mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mjaccard_sim_mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recappr/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recappr/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_item_knn(csr_matrix,compute_jaccard_sim_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9feac68-c964-48c9-a353-413fa1b2879e",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc318b5-7569-4f47-bd5b-5e16e658017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@1     : 0.359\n",
      "hit_ratio@1 : 0.359\n",
      "ndcg@3     : 0.462\n",
      "hit_ratio@3 : 0.535\n",
      "ndcg@5     : 0.489\n",
      "hit_ratio@5 : 0.602\n",
      "ndcg@10    : 0.509\n",
      "hit_ratio@10 : 0.664\n",
      "ndcg@50    : 0.519\n",
      "hit_ratio@50 : 0.702\n"
     ]
    }
   ],
   "source": [
    "evaluate_item_knn(csr_matrix,compute_cosine_sim_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a5355-944f-4ad0-98f2-e4b2324afcf7",
   "metadata": {},
   "source": [
    "### Asymmetric Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712610e9-00a0-4311-8f39-22f14a020643",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_asymmcosine_sim_mtx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16109/1425594431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_item_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_asymmcosine_sim_mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_asymmcosine_sim_mtx' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_item_knn(csr_matrix,partial(compute_asymmcosine_sim_mtx,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f26ff-e1d5-44ff-833a-beac6c146621",
   "metadata": {},
   "source": [
    "# SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd89c05-69b7-4184-84bf-b257dac9b5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 8 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 -> 9748:  60%|████████████████▊           | 5858/9748 [25:57<20:47,  3.12it/s]"
     ]
    }
   ],
   "source": [
    "from algorithms.slim_parallel import SLIM_parallel\n",
    "\n",
    "W = SLIM_parallel(sp.csc_matrix(csr_matrix),1e-1,1e-1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa2d51-e03b-4441-ac7b-f5cd6f0910f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57de48-7901-4722-9d14-9b6504199dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_slim(matrix):\n",
    "    \n",
    "    pred_mtx = matrix @ W\n",
    "    pred_mtx = pred_mtx.toarray() \n",
    "    \n",
    "    reproducible(SEED)\n",
    "    \n",
    "    evaluator = Evaluator(test_loader.dataset.n_users)\n",
    "\n",
    "    for u_idxs, i_idxs, labels in test_loader:\n",
    "\n",
    "        out = pred_mtx[u_idxs[:,None],i_idxs]\n",
    "        evaluator.eval_batch(out)\n",
    "\n",
    "    metrics_values = evaluator.get_results()\n",
    "    print_results(metrics_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ded76b-1156-4e58-8905-92e14161ef0c",
   "metadata": {},
   "source": [
    "# EASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d8c17-43db-4bec-aaa0-8acc10c48346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.ease import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac896caf-3e1c-46ae-804d-0a00731f56da",
   "metadata": {},
   "source": [
    "# Deep Matrix Factorization Models\n",
    "[paper](https://www.ijcai.org/Proceedings/2017/0447.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54483565-26c3-4fd9-8ce2-96f95c68bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dmf(nn.Module):\n",
    "    def __init__(self, matrix:sp.spmatrix, middle_dim:int = 100, final_dim:int = 64,device='cpu'):\n",
    "        \n",
    "        super(dmf, self).__init__()\n",
    "        self.matrix = matrix\n",
    "        self.n_users = matrix.shape[0]\n",
    "        self.n_items = matrix.shape[1]\n",
    "        self.middle_dim = middle_dim\n",
    "        self.final_dim = final_dim\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "        # Going with a two-layer NN\n",
    "        self.user_nn = nn.Sequential(\n",
    "            nn.Linear(self.n_items,self.middle_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.middle_dim,self.final_dim)\n",
    "        )\n",
    "        \n",
    "        self.item_nn = nn.Sequential(\n",
    "            nn.Linear(self.n_users,self.middle_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.middle_dim,self.final_dim)\n",
    "        )\n",
    "        self.cosine_fun = nn.CosineSimilarity(dim=-1)\n",
    "        \n",
    "        self.apply(general_weight_init)\n",
    "    \n",
    "    \n",
    "    def forward(self,u_idxs,i_idxs):\n",
    "        \n",
    "        # User pass\n",
    "        u_vec = self.sparse_to_device(self.matrix[u_idxs])\n",
    "        u_vec = self.user_nn(u_vec)\n",
    "        \n",
    "        # Item pass\n",
    "        # We assume negative sampling has been applied\n",
    "        i_vec = self.sparse_to_device(self.matrix[:,i_idxs.flatten()]).T\n",
    "        i_vec = self.item_nn(i_vec)\n",
    "        i_vec = i_vec.reshape(list(i_idxs.shape) + [-1])\n",
    "        \n",
    "        # Cosine \n",
    "        sim = self.cosine_fun(u_vec[:,None,:], i_vec)\n",
    "        \n",
    "        return sim\n",
    "        \n",
    "        \n",
    "    def sparse_to_device(self,array):\n",
    "        array.toarray()\n",
    "        return torch.tensor(array.toarray(),dtype=torch.float).to(self.device)\n",
    "\n",
    "def train_dmf(model:nn.Module, train_loader: torch.utils.data.DataLoader, n_epochs:int = 50,device = 'cpu'):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for u_idxs, i_idxs, labels in tqdm(train_loader):\n",
    "            u_idxs = u_idxs.to(device)\n",
    "            i_idxs = i_idxs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(u_idxs, i_idxs)\n",
    "            \n",
    "            loss = nn.BCEWithLogitsLoss()(out.flatten(),labels.flatten())\n",
    "                        \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        print(\"Epoch {} - Epoch Avg Train Loss {:.3f} \\n\".format(epoch, epoch_train_loss))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "324cd47e-b9e3-44c7-8df4-9e9ebbcfd6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0316fdb3f145648672eac88c7ad939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Epoch Avg Train Loss 0.404 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82576e994cf048ff9c2409d084312c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Epoch Avg Train Loss 0.403 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2cbdb5b5184655bd982f3009dfcc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Epoch Avg Train Loss 0.403 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c6e76eeb474d298b6d80c738c9600e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16109/2127028818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdmf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmf_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_16109/257997379.py\u001b[0m in \u001b[0;36mtrain_dmf\u001b[0;34m(model, train_loader, n_epochs, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recappr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16109/257997379.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, u_idxs, i_idxs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# User pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mu_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mu_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16109/257997379.py\u001b[0m in \u001b[0;36msparse_to_device\u001b[0;34m(self, array)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msparse_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_dmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recappr/lib/python3.9/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mcsr_todense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dmf_model = dmf(csr_matrix)\n",
    "trained_model = train_dmf(dmf_model,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c51657d5-784c-4bce-8df3-8818fa2679fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dmf(\n",
       "  (user_nn): Sequential(\n",
       "    (0): Linear(in_features=77985, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=64, bias=True)\n",
       "  )\n",
       "  (item_nn): Sequential(\n",
       "    (0): Linear(in_features=3555, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=64, bias=True)\n",
       "  )\n",
       "  (cosine_fun): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c8bcc7e8-471d-48df-9052-bea5e2d6ff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0969,  0.0921,  0.1482,  0.2880,  0.0128],\n",
       "        [-0.0506,  0.1868,  0.0706,  0.1106,  0.0961],\n",
       "        [ 0.0128, -0.0221, -0.2083,  0.0252, -0.1793],\n",
       "        [ 0.0917,  0.0296,  0.1250,  0.0412,  0.1849],\n",
       "        [ 0.1244, -0.0803, -0.1423, -0.0265,  0.0140],\n",
       "        [ 0.1159,  0.0045,  0.0973,  0.1033, -0.0479],\n",
       "        [ 0.0464,  0.1173, -0.0350,  0.0558, -0.0056],\n",
       "        [ 0.0616, -0.1558,  0.2952,  0.2151, -0.0018],\n",
       "        [ 0.0887,  0.2490,  0.1356,  0.3124,  0.1213],\n",
       "        [-0.1694, -0.0566, -0.0799, -0.1195,  0.0241]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = sp.csr_matrix(sp.random(1000,5000))\n",
    "model = dmf(sp.csr_matrix(ran))\n",
    "u_idxs = torch.randint(high=1000,size=(10,))\n",
    "i_idxs = torch.randint(high=5000,size=(10,5))\n",
    "model(u_idxs,i_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d3b4e57a-4fc1-44bf-a4b1-43078b9d7487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4164, 1614, 2889, 3548, 3035],\n",
       "        [1384, 2288,  449, 4561, 2573],\n",
       "        [2294, 3702, 1056, 2849, 4373],\n",
       "        [4383, 4243, 3606, 4536,    1],\n",
       "        [  66,  271,  970, 1679, 1848],\n",
       "        [4810, 3644,  186,  599, 4784],\n",
       "        [2940, 4984, 2397, 1612, 4747],\n",
       "        [2634, 4905, 3054,  693, 1766],\n",
       "        [  48, 1929, 2009,  475, 2511],\n",
       "        [2875,  113, 2030, 3618,  494]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    self.model.train()\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for u_idxs, i_idxs, labels in self.train_loader:\n",
    "        u_idxs = u_idxs.to(self.device)\n",
    "        i_idxs = i_idxs.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        out = self.model(u_idxs, i_idxs)\n",
    "\n",
    "        loss = self.model.module.loss_func(out, labels)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    epoch_train_loss /= len(self.train_loader)\n",
    "    print(\"Epoch {} - Epoch Avg Train Loss {:.3f} \\n\".format(epoch, epoch_train_loss))\n",
    "\n",
    "    metrics_values = self.val()\n",
    "    curr_value = metrics_values[self.optimizing_metric]\n",
    "    print('Epoch {} - Avg Val Value {:.3f} \\n'.format(epoch, curr_value))\n",
    "    tune.report({**metrics_values, 'epoch_train_loss': epoch_train_loss})\n",
    "\n",
    "    if curr_value > best_value:\n",
    "        best_value = curr_value\n",
    "        print('Epoch {} - New best model found (val value {:.3f}) \\n'.format(epoch, curr_value))\n",
    "        with tune.checkpoint_dir(0) as checkpoint_dir:\n",
    "            torch.save(self.model.module.state_dict(), os.path.join(checkpoint_dir, 'best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39796dd-1a46-4350-9b6c-5cc82395d0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09dcfe53-e006-4e24-a4f1-1b78aa7c5768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
