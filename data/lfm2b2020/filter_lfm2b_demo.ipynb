{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f93ab9",
   "metadata": {},
   "source": [
    "# Notebook to filter the LFM2b dataset\n",
    "\n",
    "Prerequisites:\n",
    "Download the files \n",
    "- [listening-events.tsv.bz2](http://www.cp.jku.at/datasets/lfm-2b/chiir/listening-events.tsv.bz2)\n",
    "- [users.tsv.bz2](http://www.cp.jku.at/datasets/lfm-2b/chiir/users.tsv.bz2)  \n",
    "\n",
    "from http://www.cp.jku.at/datasets/LFM-2b/ to your preferred directory and extract them.\n",
    "\n",
    "In case you do not want to reduce the time span of the listening events, unlike us,\n",
    "download [listening-counts.tsv.bz2](http://www.cp.jku.at/datasets/lfm-2b/chiir/listening-counts.tsv.bz2) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5cfd6",
   "metadata": {},
   "source": [
    "As the dataset is quite big (~90GB extracted), we filter all listening events that are not relevant for our use-case and will proceed with the preprocessing in another notebook.\n",
    "\n",
    "Our main objective is to reduce the memory consumption when loading the dataset, such that anyone can create the dataset for themselves. We are aware that this leads to longer processing times, however, as this has to be done only once, this should be an acceptible tradeoff. \n",
    "\n",
    "To do so, please execute the following command to split the dataset into multiple smaller files:  \n",
    "```/usr/bin/split listening-events.tsv -l 20000000```  \n",
    "and move all files into a separate folder.\n",
    "\n",
    "The current configuration requieres about 3 GB of free memory and takes ~2h to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedd13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"F:\\Temp\\data\\lfm1y\"\n",
    "splits_dir = r\"F:\\Temp\\data\\lfm1y\\splits\"\n",
    "filtered_splits_dir = r\"F:\\Temp\\data\\lfm1y\\splits_filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0181f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime,  timedelta\n",
    "from scipy import sparse as sp\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2061f",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb864bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time range of which to take interactions\n",
    "# min_time = datetime.fromisoformat('2016-01-01')\n",
    "# max_time = datetime.today()\n",
    "\n",
    "def date_parser(time_str):\n",
    "    return datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# def time_ok(time_str):\n",
    "#     return min_time <= datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\") <= max_time\n",
    "\n",
    "# filter users & tracks with too less interaction\n",
    "min_interactions_user = 5\n",
    "min_interactions_item = 5\n",
    "min_interaction_count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f70cd",
   "metadata": {},
   "source": [
    "### Preprocess users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591d2aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>RegistrationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>US</td>\n",
       "      <td>27</td>\n",
       "      <td>2005-05-24 18:42:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>2008-07-11 16:30:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>m</td>\n",
       "      <td>NL</td>\n",
       "      <td>37</td>\n",
       "      <td>2004-04-03 01:43:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>m</td>\n",
       "      <td>RU</td>\n",
       "      <td>24</td>\n",
       "      <td>2011-07-10 23:29:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>RO</td>\n",
       "      <td>23</td>\n",
       "      <td>2008-12-26 08:42:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender Country  Age     RegistrationDate\n",
       "0       0      f      US   27  2005-05-24 18:42:51\n",
       "1       1      m     NaN   32  2008-07-11 16:30:15\n",
       "2       2      m      NL   37  2004-04-03 01:43:27\n",
       "3       3      m      RU   24  2011-07-10 23:29:23\n",
       "4       4      m      RO   23  2008-12-26 08:42:52"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = pd.read_csv(os.path.join(data_dir, \"users.tsv\"), \n",
    "                        sep=\"\\t\", engine=\"python\", encoding='latin-1',\n",
    "                        header=0)\n",
    "df_users.columns = [\"UserID\", \"Gender\", \"Country\", \"Age\", \"RegistrationDate\"]\n",
    "# df_users.columns = [\"UserID\", \"Country\", \"Age\", \"Gender\", \"RegistrationDate\"]\n",
    "\n",
    "# keep users for which at least one attribute is available\n",
    "user_mask = ~df_users[\"Country\"].isna()\n",
    "user_mask |= df_users[\"Gender\"].isin([\"f\", \"m\"])\n",
    "user_mask |= df_users[\"Age\"] > 0\n",
    "\n",
    "df_users = df_users[user_mask]\n",
    "user_ids = df_users[\"UserID\"]\n",
    "\n",
    "# store old user indices and adjust user ids\n",
    "df_users.reset_index(drop=True, inplace=True)\n",
    "user_mapping = {a: b for a, b in zip(user_ids, df_users.index)}\n",
    "df_users = df_users.assign(UserID = df_users.index)\n",
    "\n",
    "df_users.to_csv(os.path.join(data_dir, \"users-demo.tsv\"), sep=\"\\t\", index=False)\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a365ba5",
   "metadata": {},
   "source": [
    "### Filter interactions\n",
    "We first select only the users for which some demographic information is available. Moreover, we drop all interactions that did happen in our previously defined time frame. The results will again be stored in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfb0994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|█████████████████████████████████████████████████████████████████| 136/136 [07:15<00:00,  3.20s/it]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(filtered_splits_dir, exist_ok=True)\n",
    "\n",
    "first = True\n",
    "fcount = 0\n",
    "header = [\"user_id\", \"item_id\", \"timestamp\"]\n",
    "\n",
    "item_indices = set()\n",
    "\n",
    "files = list(sorted(glob.glob(os.path.join(splits_dir, \"*\"))))\n",
    "for f in tqdm(files, desc=\"Parsing files\"):\n",
    "    if first:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", engine=\"c\")\n",
    "        df.columns = header\n",
    "        first = False\n",
    "    else:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", names=header, engine=\"c\")\n",
    "    \n",
    "    # Filter by user id and timestamp\n",
    "    uid_mask = df[\"user_id\"].isin(user_ids)\n",
    "#     df[\"time\"] = pd.to_datetime(df['time'], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "#     time_mask = (min_time <= df[\"time\"]) & (df[\"time\"] <= max_time)\n",
    "        \n",
    "    mask = uid_mask #& time_mask\n",
    "    if mask.sum() > 0:\n",
    "        df = df.loc[mask, [\"user_id\", \"item_id\", \"timestamp\"]]\n",
    "        df[\"user_id\"] = df[\"user_id\"].map(user_mapping)\n",
    "        \n",
    "        item_indices = item_indices.union(df[\"item_id\"].unique())\n",
    "        \n",
    "        path = os.path.join(filtered_splits_dir, f\"filtered_part_{fcount}.tsv\")\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "        fcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b5938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store indices for later, to prevent unwanted execution time when running later cells...\n",
    "with open(os.path.join(filtered_splits_dir, \"item_indices.pkl\"), \"wb\") as fh:\n",
    "    pkl.dump(item_indices, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6642b",
   "metadata": {},
   "source": [
    "### Generate interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4acabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(filtered_splits_dir, \"item_indices.pkl\"), \"rb\") as fh:\n",
    "    item_indices = pkl.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6b03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=4998, n_items=5846958\n"
     ]
    }
   ],
   "source": [
    "n_users = len(df_users)\n",
    "n_items = len(item_indices)\n",
    "print(f\"n_users={n_users}, n_items={n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87402e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mapping = {idx: i for i, idx in enumerate(item_indices)}\n",
    "user_mapping_filtered = {i: i for i in range(len(user_mapping))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2327e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mapping(initial_mapping, valid_item_indices):\n",
    "    \"\"\" Function for adjusting the mappings we keep to refer back to the old data. \"\"\"\n",
    "    keep_mapping = {iid: i for i, iid in enumerate(valid_item_indices)}\n",
    "    reverse_mapping = dict(map(reversed, initial_mapping.items()))\n",
    "    return {reverse_mapping[k]: v for k, v in keep_mapping.items()}, keep_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812ceabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|█████████████████████████████████████████████████████████████████| 136/136 [07:34<00:00,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate interaction matrix for the individual files\n",
    "files = sorted(glob.glob(os.path.join(filtered_splits_dir, \"*.tsv\")))\n",
    "for i, f in enumerate(tqdm(files, desc=\"Parsing files\")):\n",
    "    df = pd.read_csv(f, sep=\"\\t\")\n",
    "    \n",
    "    d = defaultdict(lambda: 0)\n",
    "    for uid, iid in zip(df[\"user_id\"], df[\"item_id\"]):\n",
    "        d[(uid, iid)] += 1\n",
    "        \n",
    "    uids, iids = zip(*d.keys())\n",
    "    iids = [item_mapping[i] for i in iids]\n",
    "    data = list(d.values())\n",
    "    \n",
    "    interaction_matrix = sp.csr_matrix((data, (uids, iids)), \n",
    "                                       (n_users, n_items))\n",
    "\n",
    "    sp.save_npz(os.path.join(filtered_splits_dir, f\"interaction_matrix_part_{i}.npz\"), interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1275f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing item chunks: 100%|██████████████████████████████████████████████████████████| 12/12 [00:58<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# process the items in chunks when loading the previously stored matrix parts\n",
    "items_per_chunk = 500_000\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(filtered_splits_dir, \"*.npz\")))\n",
    "all_valid_items = np.array([], dtype=int)\n",
    "\n",
    "im_full = []\n",
    "chunk_steps = list(range(0, n_items, items_per_chunk))\n",
    "for i in tqdm(chunk_steps, desc=\"Processing item chunks\"):\n",
    "    interaction_matrix = None\n",
    "    for f in files:\n",
    "        im_part = sp.load_npz(f)[:, i:i+items_per_chunk]\n",
    "        if interaction_matrix is None:\n",
    "            interaction_matrix = sp.csr_matrix(im_part.shape, dtype=int)\n",
    "        interaction_matrix += im_part\n",
    "\n",
    "    # Ignore \"misclick\" interactions\n",
    "    interaction_matrix.data -= min_interaction_count - 1\n",
    "    interaction_matrix.eliminate_zeros()\n",
    "\n",
    "    # and binarize result\n",
    "    interaction_matrix.data[:] = 1\n",
    "        \n",
    "    # determine items with enough interactions. this should already reduce the number of items considerably,\n",
    "    # allowing us to store them in memory\n",
    "    valid_items = np.argwhere(np.array(interaction_matrix.sum(axis=0)).flatten() >= min_interactions_item).flatten()\n",
    "    im_full.append(interaction_matrix[:, valid_items])\n",
    "    all_valid_items = np.concatenate([all_valid_items, valid_items + i])\n",
    "    \n",
    "im_full = sp.hstack(im_full).tocsr()\n",
    "\n",
    "# moreover, determine which users are lacking interactions.\n",
    "valid_users = np.argwhere(np.array(im_full.sum(axis=1)).flatten() >= min_interactions_user).flatten()\n",
    "im_full = im_full[valid_users, :]\n",
    "df_users = df_users.loc[valid_users, :]\n",
    "df_users.reset_index(drop=True, inplace=True)\n",
    "\n",
    "item_mapping, _ = update_mapping(item_mapping, all_valid_items)\n",
    "user_mapping_filtered, user_keep_mapping = update_mapping(user_mapping_filtered, valid_users)\n",
    "df_users[\"UserID\"] = df_users[\"UserID\"].map(user_keep_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee9d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: n_users=4962, n_items=359258\n",
      "Performing min interaction filtering - remaining n_users=4962, n_items=359258\n",
      "Performing min interaction filtering - remaining n_users=4962, n_items=359254\n",
      "After filtering: n_users=4962, n_items=359254\n"
     ]
    }
   ],
   "source": [
    "nu, ni = im_full.shape\n",
    "print(f\"Before filtering: n_users={nu}, n_items={ni}\")\n",
    "\n",
    "# filtering users may again lead to items with too few interactions, and vice-versa. \n",
    "while True:\n",
    "    nu, ni = im_full.shape\n",
    "    valid_items = np.argwhere(np.array(im_full.sum(axis=0)).flatten() >= min_interactions_item).flatten()\n",
    "    im_full = im_full[:, valid_items]\n",
    "    \n",
    "    valid_users = np.argwhere(np.array(im_full.sum(axis=1)).flatten() >= min_interactions_user).flatten()\n",
    "    im_full = im_full[valid_users, :]\n",
    "    df_users = df_users.loc[valid_users, :]\n",
    "    df_users.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    item_mapping, _ = update_mapping(item_mapping, valid_items)\n",
    "    user_mapping_filtered, user_keep_mapping = update_mapping(user_mapping_filtered, valid_users)\n",
    "    df_users[\"UserID\"] = df_users[\"UserID\"].map(user_keep_mapping)\n",
    "    print(f\"Performing min interaction filtering - remaining n_users={nu}, n_items={ni}\")\n",
    "\n",
    "    new_nu, new_ni = im_full.shape\n",
    "    if new_nu == nu and new_ni == ni:\n",
    "        break\n",
    "\n",
    "print(f\"After filtering: n_users={new_nu}, n_items={new_ni}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e64b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matrix, density=0.003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4962x359254 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 5934042 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sp.save_npz(os.path.join(data_dir, \"interaction_matrix_filtered.npz\"), im_full)\n",
    "\n",
    "df_users = df_users.loc[valid_users, :]\n",
    "df_users.reset_index(drop=True, inplace=True)\n",
    "df_users = df_users.assign(UserID=df_users.index)\n",
    "df_users.to_csv(os.path.join(data_dir, \"users_demo_filtered.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Final matrix, density={im_full.nnz / (new_nu*new_ni):.3f}\")\n",
    "display(im_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a7299",
   "metadata": {},
   "source": [
    "### Store mappings for later reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d3ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = dict(map(reversed, user_mapping.items()))\n",
    "user_mapping = {reverse_mapping[k]: v for k, v in user_mapping_filtered.items()}\n",
    "\n",
    "df_mapping = pd.DataFrame.from_dict(user_mapping, orient=\"index\")\n",
    "df_mapping.reset_index(inplace=True)\n",
    "df_mapping.columns = [\"old_user_id\", \"new_user_id\"]\n",
    "df_mapping.to_csv(os.path.join(data_dir, \"user_mapping.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c259e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = pd.DataFrame.from_dict(user_mapping_filtered, orient=\"index\")\n",
    "df_mapping.reset_index(inplace=True)\n",
    "df_mapping.columns = [\"old_user_id\", \"new_user_id\"]\n",
    "df_mapping.to_csv(os.path.join(data_dir, \"user_mapping_filtered.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db44a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = pd.DataFrame.from_dict(item_mapping, orient=\"index\")\n",
    "df_mapping.reset_index(inplace=True)\n",
    "df_mapping.columns = [\"old_item_id\", \"new_item_id\"]\n",
    "df_mapping.to_csv(os.path.join(data_dir, \"item_mapping.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccabc5",
   "metadata": {},
   "source": [
    "### Generating a new interaction_history.tsv file which includes a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc6bcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_full_coo = im_full.tocoo()\n",
    "\n",
    "uids, iids = im_full_coo.row, im_full_coo.col\n",
    "\n",
    "# set for fast (O(1)) lookup\n",
    "interactions = set([tuple(el) for el in  np.stack([uids, iids], axis=1).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0307e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|█████████████████████████████████████████████████████████████████| 136/136 [06:00<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "full_df = None\n",
    "\n",
    "# generate interaction matrix for the individual files\n",
    "files = sorted(glob.glob(os.path.join(filtered_splits_dir, \"*.tsv\")))\n",
    "for i, f in enumerate(tqdm(files, desc=\"Parsing files\")):\n",
    "    \n",
    "    df = pd.read_csv(f, sep=\"\\t\")\n",
    "    n_inter = len(df)\n",
    "    \n",
    "    # adjust user and item id's\n",
    "    # Note: for elements that are not in the dictionaries, the result will be NaN\n",
    "    df.user_id = df.user_id.map(user_mapping_filtered)\n",
    "    df.item_id = df.item_id.map(item_mapping)\n",
    "\n",
    "    # by dropping all entries, where either user or item was filtered due to not enough interactions,\n",
    "    # we only keep interacitons that are in the interactions matrix\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # not sure why, but ids are floats here (dict mapping is int -> int)...\n",
    "    df.user_id = df.user_id.astype(int)\n",
    "    df.item_id = df.item_id.astype(int)\n",
    "    \n",
    "    # find first interaction by first ensuring that dataframe is sorted\n",
    "    df.sort_values(\"timestamp\", inplace=True)\n",
    "    \n",
    "    # we switch to numpy methods to get first timestamp of interaction to speed up compuations\n",
    "    # check out https://stackoverflow.com/a/38145104 for some nice extra information\n",
    "    arr_slice = df[[\"user_id\", \"item_id\"]].values\n",
    "    lidx = np.ravel_multi_index(arr_slice.T, arr_slice.max(0)+1)\n",
    "    \n",
    "    # this retrieves the first indices for every (user,item) interaction\n",
    "    _, indices = np.unique(lidx, return_index=True)\n",
    "    df = df.iloc[indices]\n",
    "    \n",
    "    # collect all partial DataFrames\n",
    "    full_df = df if full_df is None else pd.concat([full_df, df])\n",
    "        \n",
    "# filtering a final time, now that all history parts are joined together\n",
    "df = full_df\n",
    "df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "arr_slice = df[[\"user_id\", \"item_id\"]].values\n",
    "lidx = np.ravel_multi_index(arr_slice.T, arr_slice.max(0)+1)\n",
    "_, indices = np.unique(lidx, return_index=True)\n",
    "df = df.iloc[indices]\n",
    "\n",
    "# dropping bad interactions\n",
    "df = df.assign(ui=list(tuple(el) for el in np.stack([df.user_id, df.item_id], axis=1)))\n",
    "df = df.loc[df[\"ui\"].isin(interactions)].drop(columns=[\"ui\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c82bb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25695</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>2019-04-22 04:25:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290113</th>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>2019-08-22 17:38:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181817</th>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>2019-04-22 23:45:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478737</th>\n",
       "      <td>0</td>\n",
       "      <td>352</td>\n",
       "      <td>2019-07-07 18:16:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498331</th>\n",
       "      <td>0</td>\n",
       "      <td>412</td>\n",
       "      <td>2019-07-04 18:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409031</th>\n",
       "      <td>4961</td>\n",
       "      <td>349834</td>\n",
       "      <td>2019-03-30 00:28:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531500</th>\n",
       "      <td>4961</td>\n",
       "      <td>349835</td>\n",
       "      <td>2019-03-30 18:18:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410349</th>\n",
       "      <td>4961</td>\n",
       "      <td>349836</td>\n",
       "      <td>2019-03-30 00:38:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407967</th>\n",
       "      <td>4961</td>\n",
       "      <td>349838</td>\n",
       "      <td>2019-03-30 00:19:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407638</th>\n",
       "      <td>4961</td>\n",
       "      <td>349839</td>\n",
       "      <td>2019-03-30 00:17:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5934042 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id            timestamp\n",
       "25695         0      106  2019-04-22 04:25:07\n",
       "290113        0      281  2019-08-22 17:38:29\n",
       "181817        0      327  2019-04-22 23:45:57\n",
       "478737        0      352  2019-07-07 18:16:07\n",
       "498331        0      412  2019-07-04 18:05:00\n",
       "...         ...      ...                  ...\n",
       "409031     4961   349834  2019-03-30 00:28:24\n",
       "531500     4961   349835  2019-03-30 18:18:37\n",
       "410349     4961   349836  2019-03-30 00:38:56\n",
       "407967     4961   349838  2019-03-30 00:19:43\n",
       "407638     4961   349839  2019-03-30 00:17:06\n",
       "\n",
       "[5934042 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04714e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(data_dir, \"interaction_history_filtered.tsv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "268f1bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4962x359254 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 5934042 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6229881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All interactions with their first timestep are maintained in the new dataframe:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"All interactions with their first timestep are maintained in the new dataframe:\")\n",
    "print(len(df) == im_full.nnz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
